# dfTest1 = drop(dfTest1, 'testY')     # remove testY from dfTest1
# Orginal Model
M1 = lm(Profit~Runtime + awardSum + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoRating + tomatoFresh + tomatoRotten + tomatoUserMeter +  tomatoUserReviews + Budget, dfTrain);
################################## ADJUST MODEL HERE #####################################
#columns to use in lm
#lmColNames = paste(names(x), collapse=' + ')
#lmColNames = paste(names(x), collapse=' , ')
# Title + Year + Rated + Released + Runtime + Genre + Director + Writer + Actors + Language + Country + Awards + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoImage + tomatoRating + tomatoReviews + tomatoFresh + tomatoRotten + tomatoUserMeter + tomatoUserRating + tomatoUserReviews + DVD + Production + Budget + Date + awardSum + awardCategory + action + adult + adventure + animation + biography + comedy + crime + documentary + drama + family + fantasy + history + horror + music + musical + mystery + n.a + news + romance + sci.fi + short + sport + thriller + war + wester
# New model
M2 <- cv.glmnet(X_train,  ## inputs
Y_train,  ## target
family = "gaussian",   ## linear regression
alpha = 1,   ## select Lasso
type.measure = "mse",  ## train to minimize mse
nfolds = 5,    ## 5-folds cross-validation
parallel = TRUE)  ## done in parallel
# So we can see that the best lambda was selected such that the red curve is minimum but the error bars are quite big!
# and with LASSO
trainPredict <- predict(M2, newx = X_train, s = "lambda.min")
prediction <- predict(M2, newx = X_test, s = "lambda.min")
#M2 = lm(Profit ~ ., dfTrain1);
#theta<-coef(M1)
#summary(M1)$r.squared
MSEtrain = MSEfun(M1)
MSEtrain2 = mean((trainPredict - Y_train)^2)
MSEtest = mean((predict(M1, dfTest) - testY)^2)
#MSEtest2 = mean((predict(M2, dfTest1) - testY)^2)
MSEtest2 = mean((prediction - Y_test)^2)
MSEtrainList = c(MSEtrainList, MSEtrain)
MSEtestList = c(MSEtestList, MSEtest)
MSEtrainList2 = c(MSEtrainList2, MSEtrain2)
MSEtestList2 = c(MSEtestList2, MSEtest2)
repetitions = repetitions - 1
} # end while loop
MSEdf = data.frame(N = N,
trainError = MSEtrainList,
testError = MSEtestList,
trainError2 = MSEtrainList2,
testError2 = MSEtestList2)
print(colMeans(MSEdf))
#Repeat the random partition of 70% and 30% 10 times and average the test accuracy results over the 10 repetitions.
MSEAvTrainList = c(MSEAvTrainList, mean(MSEdf$trainError))
MSEAvTestList = c(MSEAvTestList, mean(MSEdf$testError))
MSEAvTrainList2 = c(MSEAvTrainList2, mean(MSEdf$trainError2))
MSEAvTestList2 = c(MSEAvTestList2, mean(MSEdf$testError2))
} # end for loop
MSEAvDF = data.frame(trainSetSize = trunc(trainSeq * r/100),
trainMSE = MSEAvTrainList,
testMSE = MSEAvTestList,
trainMSE2 = MSEAvTrainList2,
testMSE2 = MSEAvTestList2)
MSEAvDF
ggplot(MSEAvDF, aes(trainSetSize)) +
geom_line(aes(y = trainMSE, color = "OldTrainMSE")) +
geom_line(aes(y = testMSE, color = "OldTestMSE")) +
geom_line(aes(y = trainMSE2, color = "NewTrainMSE")) +
geom_line(aes(y = testMSE2, color = "NewTestMSE")) +
xlab("Train Set Size") +
ylab("MSE") +
ggtitle("Q4 Average Training and Testing MSE vs. Train Set Size")  # +  coord_cartesian(ylim = c(5e+15,1.5e+16))
r = nrow(x)
MSEAvTrainList = c()
MSEAvTestList = c()
MSEAvTrainList2 = c()
MSEAvTestList2 = c()
#train-test split variable
#trainSeq = seq(5, 95, length = 19)
# shorter trainSeq
trainSeq = trunc(seq(5, 95, length = 10))
for (splitVar in trainSeq) {
cat("\n\n split: ", splitVar, "%, ", 100 - splitVar, "%\n\n")
MSEtrainList = c()
MSEtestList = c()
MSEtrainList2 = c()
MSEtestList2 = c()
repetitions = 1 #############ADJUST REPETITIONS TO 10 ################
N = seq(1, repetitions)
while (repetitions > 0) {
#Randomly divide the rows into two sets of sizes 5% and 95%
rand_perm = sample(r, r)
first_set_of_indices = rand_perm[1:floor(r * splitVar / 100)]
second_set_of_indices = rand_perm[(floor(r * splitVar / 100) + 1):r]
trainX = x[first_set_of_indices, ]
testX = x[second_set_of_indices, ]
# trainX4 = X[first_set_of_indices, ]
# testX4 = X[second_set_of_indices, ]
trainY = y[first_set_of_indices,]
testY = y[second_set_of_indices,]
Y_train <- Y[samp_train]
Y_test <- Y[-samp_train]
X_train <- X[samp_train,]
X_test <- X[-samp_train,]
#and train on the training set
dfTrain = as.data.frame(cbind(trainX, trainY))
colnames(dfTrain)[length(dfTrain)] = "Profit"
dfTest = as.data.frame(cbind(testX, testY))
dfTrain1 = select(dfTrain, Runtime , awardSum , awardCategory, Metascore , imdbRating , imdbVotes , tomatoMeter , tomatoRating , tomatoFresh , tomatoRotten , tomatoUserMeter , tomatoUserReviews, Budget, Profit)
dfTrain1 = na.omit(dfTrain1)
dfTest1 = select(dfTest, Runtime , awardSum , awardCategory, Metascore , imdbRating , imdbVotes , tomatoMeter , tomatoRating , tomatoFresh , tomatoRotten , tomatoUserMeter , tomatoUserReviews, Budget, testY)
dfTest1 = na.omit(dfTest1)
# ensure testY has the same number of datapoints as testX
testY = dfTest1$testY
dfTest1 = drop(dfTest1, 'testY')     # remove testY from dfTest1
# dfTest1 = as.data.frame(cbind(testX4, testY))
# Compute the MSE on the train and test sets (normalize the MSE by the number of samples)
# Title + Year + Rated + Released + Runtime + Genre + Director + Writer + Actors + Language + Country + Awards + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoImage + tomatoRating + tomatoReviews + tomatoFresh + tomatoRotten + tomatoUserMeter + tomatoUserRating + tomatoUserReviews + DVD + Production + Budget + Date
# dfTrain1 = na.omit(dfTrain1)
#
# dfTest1 = na.omit(dfTest1)
# ensure testY has the same number of datapoints as testX
# testY = dfTest1$testY
# dfTest1 = drop(dfTest1, 'testY')     # remove testY from dfTest1
# Orginal Model
M1 = lm(Profit~Runtime + awardSum + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoRating + tomatoFresh + tomatoRotten + tomatoUserMeter +  tomatoUserReviews + Budget, dfTrain1);
################################## ADJUST MODEL HERE #####################################
#columns to use in lm
#lmColNames = paste(names(x), collapse=' + ')
#lmColNames = paste(names(x), collapse=' , ')
# Title + Year + Rated + Released + Runtime + Genre + Director + Writer + Actors + Language + Country + Awards + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoImage + tomatoRating + tomatoReviews + tomatoFresh + tomatoRotten + tomatoUserMeter + tomatoUserRating + tomatoUserReviews + DVD + Production + Budget + Date + awardSum + awardCategory + action + adult + adventure + animation + biography + comedy + crime + documentary + drama + family + fantasy + history + horror + music + musical + mystery + n.a + news + romance + sci.fi + short + sport + thriller + war + wester
# New model
M2 <- cv.glmnet(X_train,  ## inputs
Y_train,  ## target
family = "gaussian",   ## linear regression
alpha = 1,   ## select Lasso
type.measure = "mse",  ## train to minimize mse
nfolds = 5,    ## 5-folds cross-validation
parallel = TRUE)  ## done in parallel
# So we can see that the best lambda was selected such that the red curve is minimum but the error bars are quite big!
# and with LASSO
trainPredict <- predict(M2, newx = X_train, s = "lambda.min")
prediction <- predict(M2, newx = X_test, s = "lambda.min")
#M2 = lm(Profit ~ ., dfTrain1);
#theta<-coef(M1)
#summary(M1)$r.squared
MSEtrain = MSEfun(M1)
MSEtrain2 = mean((trainPredict - Y_train)^2)
MSEtest = mean((predict(M1, dfTest1) - testY)^2)
#MSEtest2 = mean((predict(M2, dfTest1) - testY)^2)
MSEtest2 = mean((prediction - Y_test)^2)
MSEtrainList = c(MSEtrainList, MSEtrain)
MSEtestList = c(MSEtestList, MSEtest)
MSEtrainList2 = c(MSEtrainList2, MSEtrain2)
MSEtestList2 = c(MSEtestList2, MSEtest2)
repetitions = repetitions - 1
} # end while loop
MSEdf = data.frame(N = N,
trainError = MSEtrainList,
testError = MSEtestList,
trainError2 = MSEtrainList2,
testError2 = MSEtestList2)
print(colMeans(MSEdf))
#Repeat the random partition of 70% and 30% 10 times and average the test accuracy results over the 10 repetitions.
MSEAvTrainList = c(MSEAvTrainList, mean(MSEdf$trainError))
MSEAvTestList = c(MSEAvTestList, mean(MSEdf$testError))
MSEAvTrainList2 = c(MSEAvTrainList2, mean(MSEdf$trainError2))
MSEAvTestList2 = c(MSEAvTestList2, mean(MSEdf$testError2))
} # end for loop
MSEAvDF = data.frame(trainSetSize = trunc(trainSeq * r/100),
trainMSE = MSEAvTrainList,
testMSE = MSEAvTestList,
trainMSE2 = MSEAvTrainList2,
testMSE2 = MSEAvTestList2)
MSEAvDF
ggplot(MSEAvDF, aes(trainSetSize)) +
geom_line(aes(y = trainMSE, color = "OldTrainMSE")) +
geom_line(aes(y = testMSE, color = "OldTestMSE")) +
geom_line(aes(y = trainMSE2, color = "NewTrainMSE")) +
geom_line(aes(y = testMSE2, color = "NewTestMSE")) +
xlab("Train Set Size") +
ylab("MSE") +
ggtitle("Q4 Average Training and Testing MSE vs. Train Set Size")  # +  coord_cartesian(ylim = c(5e+15,1.5e+16))
r = nrow(x)
MSEAvTrainList = c()
MSEAvTestList = c()
MSEAvTrainList2 = c()
MSEAvTestList2 = c()
#train-test split variable
#trainSeq = seq(5, 95, length = 19)
# shorter trainSeq
trainSeq = trunc(seq(5, 95, length = 10))
for (splitVar in trainSeq) {
cat("\n\n split: ", splitVar, "%, ", 100 - splitVar, "%\n\n")
MSEtrainList = c()
MSEtestList = c()
MSEtrainList2 = c()
MSEtestList2 = c()
repetitions = 1 #############ADJUST REPETITIONS TO 10 ################
N = seq(1, repetitions)
while (repetitions > 0) {
#Randomly divide the rows into two sets of sizes 5% and 95%
rand_perm = sample(r, r)
first_set_of_indices = rand_perm[1:floor(r * splitVar / 100)]
second_set_of_indices = rand_perm[(floor(r * splitVar / 100) + 1):r]
trainX = x[first_set_of_indices, ]
testX = x[second_set_of_indices, ]
# trainX4 = X[first_set_of_indices, ]
# testX4 = X[second_set_of_indices, ]
trainY = y[first_set_of_indices,]
testY = y[second_set_of_indices,]
Y_train <- Y[samp_train]
Y_test <- Y[-samp_train]
X_train <- X[samp_train,]
X_test <- X[-samp_train,]
#and train on the training set
dfTrain = as.data.frame(cbind(trainX, trainY))
colnames(dfTrain)[length(dfTrain)] = "Profit"
dfTest = as.data.frame(cbind(testX, testY))
dfTrain1 = select(dfTrain, Runtime , awardSum , Metascore , imdbRating , imdbVotes , tomatoMeter , tomatoRating , tomatoFresh , tomatoRotten , tomatoUserMeter , tomatoUserReviews, Budget, Profit)
dfTrain1 = na.omit(dfTrain1)
dfTest1 = select(dfTest, Runtime , awardSum ,  Metascore , imdbRating , imdbVotes , tomatoMeter , tomatoRating , tomatoFresh , tomatoRotten , tomatoUserMeter , tomatoUserReviews, Budget, testY)
dfTest1 = na.omit(dfTest1)
# ensure testY has the same number of datapoints as testX
testY = dfTest1$testY
dfTest1 = drop(dfTest1, 'testY')     # remove testY from dfTest1
# dfTest1 = as.data.frame(cbind(testX4, testY))
# Compute the MSE on the train and test sets (normalize the MSE by the number of samples)
# Title + Year + Rated + Released + Runtime + Genre + Director + Writer + Actors + Language + Country + Awards + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoImage + tomatoRating + tomatoReviews + tomatoFresh + tomatoRotten + tomatoUserMeter + tomatoUserRating + tomatoUserReviews + DVD + Production + Budget + Date
# dfTrain1 = na.omit(dfTrain1)
#
# dfTest1 = na.omit(dfTest1)
# ensure testY has the same number of datapoints as testX
# testY = dfTest1$testY
# dfTest1 = drop(dfTest1, 'testY')     # remove testY from dfTest1
# Orginal Model
M1 = lm(Profit~Runtime + awardSum + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoRating + tomatoFresh + tomatoRotten + tomatoUserMeter +  tomatoUserReviews + Budget, dfTrain1);
################################## ADJUST MODEL HERE #####################################
#columns to use in lm
#lmColNames = paste(names(x), collapse=' + ')
#lmColNames = paste(names(x), collapse=' , ')
# Title + Year + Rated + Released + Runtime + Genre + Director + Writer + Actors + Language + Country + Awards + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoImage + tomatoRating + tomatoReviews + tomatoFresh + tomatoRotten + tomatoUserMeter + tomatoUserRating + tomatoUserReviews + DVD + Production + Budget + Date + awardSum + awardCategory + action + adult + adventure + animation + biography + comedy + crime + documentary + drama + family + fantasy + history + horror + music + musical + mystery + n.a + news + romance + sci.fi + short + sport + thriller + war + wester
# New model
M2 <- cv.glmnet(X_train,  ## inputs
Y_train,  ## target
family = "gaussian",   ## linear regression
alpha = 1,   ## select Lasso
type.measure = "mse",  ## train to minimize mse
nfolds = 5,    ## 5-folds cross-validation
parallel = TRUE)  ## done in parallel
# So we can see that the best lambda was selected such that the red curve is minimum but the error bars are quite big!
# and with LASSO
trainPredict <- predict(M2, newx = X_train, s = "lambda.min")
prediction <- predict(M2, newx = X_test, s = "lambda.min")
#M2 = lm(Profit ~ ., dfTrain1);
#theta<-coef(M1)
#summary(M1)$r.squared
MSEtrain = MSEfun(M1)
MSEtrain2 = mean((trainPredict - Y_train)^2)
MSEtest = mean((predict(M1, dfTest1) - testY)^2)
#MSEtest2 = mean((predict(M2, dfTest1) - testY)^2)
MSEtest2 = mean((prediction - Y_test)^2)
MSEtrainList = c(MSEtrainList, MSEtrain)
MSEtestList = c(MSEtestList, MSEtest)
MSEtrainList2 = c(MSEtrainList2, MSEtrain2)
MSEtestList2 = c(MSEtestList2, MSEtest2)
repetitions = repetitions - 1
} # end while loop
MSEdf = data.frame(N = N,
trainError = MSEtrainList,
testError = MSEtestList,
trainError2 = MSEtrainList2,
testError2 = MSEtestList2)
print(colMeans(MSEdf))
#Repeat the random partition of 70% and 30% 10 times and average the test accuracy results over the 10 repetitions.
MSEAvTrainList = c(MSEAvTrainList, mean(MSEdf$trainError))
MSEAvTestList = c(MSEAvTestList, mean(MSEdf$testError))
MSEAvTrainList2 = c(MSEAvTrainList2, mean(MSEdf$trainError2))
MSEAvTestList2 = c(MSEAvTestList2, mean(MSEdf$testError2))
} # end for loop
MSEAvDF = data.frame(trainSetSize = trunc(trainSeq * r/100),
trainMSE = MSEAvTrainList,
testMSE = MSEAvTestList,
trainMSE2 = MSEAvTrainList2,
testMSE2 = MSEAvTestList2)
MSEAvDF
ggplot(MSEAvDF, aes(trainSetSize)) +
geom_line(aes(y = trainMSE, color = "OldTrainMSE")) +
geom_line(aes(y = testMSE, color = "OldTestMSE")) +
geom_line(aes(y = trainMSE2, color = "NewTrainMSE")) +
geom_line(aes(y = testMSE2, color = "NewTestMSE")) +
xlab("Train Set Size") +
ylab("MSE") +
ggtitle("Q4 Average Training and Testing MSE vs. Train Set Size")  # +  coord_cartesian(ylim = c(5e+15,1.5e+16))
r = nrow(x)
MSEAvTrainList = c()
MSEAvTestList = c()
MSEAvTrainList2 = c()
MSEAvTestList2 = c()
#train-test split variable
#trainSeq = seq(5, 95, length = 19)
# shorter trainSeq
trainSeq = trunc(seq(5, 95, length = 10))
for (splitVar in trainSeq) {
cat("\n\n split: ", splitVar, "%, ", 100 - splitVar, "%\n\n")
MSEtrainList = c()
MSEtestList = c()
MSEtrainList2 = c()
MSEtestList2 = c()
repetitions = 10 #############ADJUST REPETITIONS TO 10 ################
N = seq(1, repetitions)
while (repetitions > 0) {
#Randomly divide the rows into two sets of sizes 5% and 95%
rand_perm = sample(r, r)
first_set_of_indices = rand_perm[1:floor(r * splitVar / 100)]
second_set_of_indices = rand_perm[(floor(r * splitVar / 100) + 1):r]
trainX = x[first_set_of_indices, ]
testX = x[second_set_of_indices, ]
# trainX4 = X[first_set_of_indices, ]
# testX4 = X[second_set_of_indices, ]
trainY = y[first_set_of_indices,]
testY = y[second_set_of_indices,]
Y_train <- Y[samp_train]
Y_test <- Y[-samp_train]
X_train <- X[samp_train,]
X_test <- X[-samp_train,]
#and train on the training set
dfTrain = as.data.frame(cbind(trainX, trainY))
colnames(dfTrain)[length(dfTrain)] = "Profit"
dfTest = as.data.frame(cbind(testX, testY))
dfTrain1 = select(dfTrain, Runtime , awardSum , Metascore , imdbRating , imdbVotes , tomatoMeter , tomatoRating , tomatoFresh , tomatoRotten , tomatoUserMeter , tomatoUserReviews, Budget, Profit)
dfTrain1 = na.omit(dfTrain1)
dfTest1 = select(dfTest, Runtime , awardSum ,  Metascore , imdbRating , imdbVotes , tomatoMeter , tomatoRating , tomatoFresh , tomatoRotten , tomatoUserMeter , tomatoUserReviews, Budget, testY)
dfTest1 = na.omit(dfTest1)
# ensure testY has the same number of datapoints as testX
testY = dfTest1$testY
dfTest1 = drop(dfTest1, 'testY')     # remove testY from dfTest1
# dfTest1 = as.data.frame(cbind(testX4, testY))
# Compute the MSE on the train and test sets (normalize the MSE by the number of samples)
# Title + Year + Rated + Released + Runtime + Genre + Director + Writer + Actors + Language + Country + Awards + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoImage + tomatoRating + tomatoReviews + tomatoFresh + tomatoRotten + tomatoUserMeter + tomatoUserRating + tomatoUserReviews + DVD + Production + Budget + Date
# dfTrain1 = na.omit(dfTrain1)
#
# dfTest1 = na.omit(dfTest1)
# ensure testY has the same number of datapoints as testX
# testY = dfTest1$testY
# dfTest1 = drop(dfTest1, 'testY')     # remove testY from dfTest1
# Orginal Model
M1 = lm(Profit~Runtime + awardSum + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoRating + tomatoFresh + tomatoRotten + tomatoUserMeter +  tomatoUserReviews + Budget, dfTrain1);
################################## ADJUST MODEL HERE #####################################
#columns to use in lm
#lmColNames = paste(names(x), collapse=' + ')
#lmColNames = paste(names(x), collapse=' , ')
# Title + Year + Rated + Released + Runtime + Genre + Director + Writer + Actors + Language + Country + Awards + Metascore + imdbRating + imdbVotes + tomatoMeter + tomatoImage + tomatoRating + tomatoReviews + tomatoFresh + tomatoRotten + tomatoUserMeter + tomatoUserRating + tomatoUserReviews + DVD + Production + Budget + Date + awardSum + awardCategory + action + adult + adventure + animation + biography + comedy + crime + documentary + drama + family + fantasy + history + horror + music + musical + mystery + n.a + news + romance + sci.fi + short + sport + thriller + war + wester
# New model
M2 <- cv.glmnet(X_train,  ## inputs
Y_train,  ## target
family = "gaussian",   ## linear regression
alpha = 1,   ## select Lasso
type.measure = "mse",  ## train to minimize mse
nfolds = 5,    ## 5-folds cross-validation
parallel = TRUE)  ## done in parallel
# So we can see that the best lambda was selected such that the red curve is minimum but the error bars are quite big!
# and with LASSO
trainPredict <- predict(M2, newx = X_train, s = "lambda.min")
prediction <- predict(M2, newx = X_test, s = "lambda.min")
#M2 = lm(Profit ~ ., dfTrain1);
#theta<-coef(M1)
#summary(M1)$r.squared
MSEtrain = MSEfun(M1)
MSEtrain2 = mean((trainPredict - Y_train)^2)
MSEtest = mean((predict(M1, dfTest1) - testY)^2)
#MSEtest2 = mean((predict(M2, dfTest1) - testY)^2)
MSEtest2 = mean((prediction - Y_test)^2)
MSEtrainList = c(MSEtrainList, MSEtrain)
MSEtestList = c(MSEtestList, MSEtest)
MSEtrainList2 = c(MSEtrainList2, MSEtrain2)
MSEtestList2 = c(MSEtestList2, MSEtest2)
repetitions = repetitions - 1
} # end while loop
MSEdf = data.frame(N = N,
trainError = MSEtrainList,
testError = MSEtestList,
trainError2 = MSEtrainList2,
testError2 = MSEtestList2)
print(colMeans(MSEdf))
#Repeat the random partition of 70% and 30% 10 times and average the test accuracy results over the 10 repetitions.
MSEAvTrainList = c(MSEAvTrainList, mean(MSEdf$trainError))
MSEAvTestList = c(MSEAvTestList, mean(MSEdf$testError))
MSEAvTrainList2 = c(MSEAvTrainList2, mean(MSEdf$trainError2))
MSEAvTestList2 = c(MSEAvTestList2, mean(MSEdf$testError2))
} # end for loop
MSEAvDF = data.frame(trainSetSize = trunc(trainSeq * r/100),
trainMSE = MSEAvTrainList,
testMSE = MSEAvTestList,
trainMSE2 = MSEAvTrainList2,
testMSE2 = MSEAvTestList2)
MSEAvDF
ggplot(MSEAvDF, aes(trainSetSize)) +
geom_line(aes(y = trainMSE, color = "OldTrainMSE")) +
geom_line(aes(y = testMSE, color = "OldTestMSE")) +
geom_line(aes(y = trainMSE2, color = "NewTrainMSE")) +
geom_line(aes(y = testMSE2, color = "NewTestMSE")) +
xlab("Train Set Size") +
ylab("MSE") +
ggtitle("Q4 Average Training and Testing MSE vs. Train Set Size")  # +  coord_cartesian(ylim = c(5e+15,1.5e+16))
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(tidyr)
load('movies_merged')
# create copy to modify
movieOnly=data.frame(movies_merged)
#Remove all rows that do not correspond to movies.
movieOnly= subset(movieOnly, Type == "movie")
# omit all rows in which gross and budget are not available.
# remove Gross NA columns
#convert "N/A" to NA
movieOnly[movieOnly == "N/A"] = NA
movieOnly = movieOnly[!is.na(movieOnly$Gross),]
movieOnly = movieOnly[!is.na(movieOnly$Budget),]
#remove all movies released prior to 2000:
movieOnly = movieOnly[(movieOnly$Year)>=2000,]
# From Project 1
#Write R code to convert Runtime to a numeric value (in minutes)
#create new dataframe to copy Runtime to
runTime=data.frame(movieOnly$Runtime)
# separate runtime column into minutes and min
runTimeSep = separate(runTime, "movieOnly.Runtime", c("hours", "h", "minutes", "min"), " ")
hoursCol = as.numeric(runTimeSep$hours) # extract the hours digits and convert to numeric values
runTimeSep$hours = hoursCol
minutesCol = as.numeric(runTimeSep$minutes) # extract the minute digits and convert to numeric values
runTimeSep$minutes = minutesCol
#convert hours to minutes
runTimeSep$newMins = with(runTimeSep, (h=="h")*60*hours + (h=="min")*hours + ifelse(is.na(minutes), 0, minutes))
movieOnly$Runtime = as.numeric(runTimeSep$newMins) #replace runtime with  the new numeric column
# remove unneeded Data
remove(minutesCol)
remove(hoursCol)
remove(awardSumDF)
# From Project 1 Code: Convert Awards to a numeric column as a total # of awards and nominations
#create new dataframe to copy awardsDF to
library(stringr)
# extract numbers
inputX = movieOnly$Awards
x = (regmatches(inputX, gregexpr('\\(?[0-9]+', inputX)))
x = lapply(x, as.numeric) # convert to numeric
xsum = lapply(x, sum) # sum up each row
awardSumDF = data.frame(unlist(xsum))
remove(xsum)
# rename columns
awardSumDF = plyr::rename(awardSumDF, c("unlist.xsum." = "awardSum"))
movieOnly = dplyr::bind_cols(movieOnly, awardSumDF) # merge dataframes
# Drop columns we don't need:
dropList = c('Gross', 'Domestic_Gross', 'BoxOffice', 'imdbID', 'Plot', 'Poster', 'Website', 'tomatoURL', 'tomatoConsensus', 'Response', 'Type')
# function to drop column names in x
drop = function(df, x) {
return(df[ , !(names(df) %in% x)])
}
movieDrop = drop(movieOnly, dropList)
#convert "N/A" to NA
movieDrop[movieDrop == "N/A"] = NA
#convert metascore to numeric
movieDrop$Metascore = as.numeric(movieDrop$Metascore)
# MSE function
# input: the output of lm, eg. M1 = lm(price~carat, diamSmall);
MSEfun = function(M1) {
return(mean(residuals(M1)^2))
}
# predict the profit associated with a movie, defined as gross revenue minus budget.
# calculate profit
y = as.data.frame(as.numeric(movieOnly$Gross - movieOnly$Budget))
colnames(y)[1] = "Profit" # rename column name
x = movieDrop
remove(movies_merged)
##########Q5##################
######Use binning from Proj1 ##################################
convertFun = function(x) {
# upperThreshold = meanAwards + sdAwards*.5
# set thresholds
t1=60
t2=140
if (x == 0) {x = "none"}
else if (x >=1 & x<=t1) {x="some"}
else if (x >t1 & x<=t2) {x="many"}
else if (x >t2 ) {x="hundreds"}
}
xsum = x$awardSum
categorized = lapply(xsum, convertFun)
categorizedDF = data.frame(unlist(categorized))
# rename columns
categorizedDF = plyr::rename(categorizedDF, c("unlist.categorized."="awardCategory"))
x = dplyr::bind_cols(x, categorizedDF) # merge dataframes
dropList5 = c("Released", "Rated", "Genre", "Director",          "Writer",            "Actors", "Language",          "Country", "tomatoImage", "Production", "Awards")
movieDrop5 = drop(movieDrop, dropList5)
#merge original DF for correct dropped rows from X
x5 = merge(movieDrop5, moviesBin, by = c("Title"))
x5 = na.omit(x5)
x5 = merge(x5, x[c("Title", "awardCategory")], by = "Title")
y5 = x5["Profit"]
xy5 = x5
x5 = drop(x5, "Profit") # remove profit from x5
# drop title from xy5 and x5
x5 = drop(x5, "Title")
xy5 = drop(xy5, "Title")
## inputs into sparse format
# library(Matrix)
#
# x5matrix<-as.matrix(x5)
# # Convert to numeric (by arbitrarily map the characters to numbers.)
# x5matrix<-sapply(data.frame(x5matrix),as.numeric)
# X = Matrix(as.matrix(x5matrix), sparse = TRUE)
# remove(x5matrix)
########### NOT USING LASSO #############
# if not using lasso
#X = moviesBin[!names(moviesBin) %in% c("Title")]
# I like to register a multicore backend to parallelize my training
library(glmnet)
## to parallelize
library(doMC)
## to detect the number of cores on your machine
library(parallel)
ncores <- detectCores()
registerDoMC(cores=ncores)
